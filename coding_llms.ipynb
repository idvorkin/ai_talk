{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f14dcd-8e57-48d7-a3be-a0eec0383d93",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Coding with LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db2cdd-dbff-4a62-8824-ceb669184b9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ooodles of Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from icecream import ic\n",
    "from rich.console import Console\n",
    "from rich import print\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "import pudb\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "console = Console()\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Any, Optional\n",
    "from langchain.output_parsers.openai_functions import OutputFunctionsParser\n",
    "from langchain.schema import FunctionMessage\n",
    "\n",
    "\n",
    "from langchain.schema import (\n",
    "    Generation,\n",
    "    OutputParserException,\n",
    ")\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a8207-04ab-4ba5-8627-74a3539ec2e5",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful helpers\n",
    "def model_to_openai_function(cls):\n",
    "    return {\"name\": cls.__name__, \"parameters\": cls.model_json_schema()}\n",
    "\n",
    "\n",
    "class JsonOutputFunctionsParser2(OutputFunctionsParser):\n",
    "    \"\"\"Parse an output as the Json object.\"\"\"\n",
    "\n",
    "    def parse_result(self, result: List[Generation]) -> Any:\n",
    "        function_call_info = super().parse_result(result)\n",
    "        if self.args_only:\n",
    "            try:\n",
    "                # Waiting for this to merge upstream\n",
    "                return json.loads(function_call_info, strict=False)\n",
    "            except (json.JSONDecodeError, TypeError) as exc:\n",
    "                raise OutputParserException(\n",
    "                    f\"Could not parse function call data: {exc}\"\n",
    "                )\n",
    "        function_call_info[\"arguments\"] = json.loads(function_call_info[\"arguments\"])\n",
    "        return function_call_info\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def print_line():\n",
    "    display(HTML(\"<hr>\"))\n",
    "\n",
    "\n",
    "def print_prompt(prompt):\n",
    "    print(\"Prompt:\")\n",
    "    for m in prompt.messages:\n",
    "        print(f\"{type(m)}  {m.prompt}\")\n",
    "\n",
    "\n",
    "def print_function_call(response):\n",
    "    # #     additional_kwargs={'function_call': {'name': 'GetWeather', 'arguments': '{\\n\"City\": \"Spain\"\\n}'}},\n",
    "    function = response.additional_kwargs[\"function_call\"]\n",
    "    print_line()\n",
    "    print(f\"Call: {function['name']}\")\n",
    "    print(function[\"arguments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3014a-0e96-4731-a244-58c50db48608",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Langchain - Super cool, we'll use it, but not our focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3ee05-94d2-44b3-a8d9-c794c48a0904",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prompts And Models - CPU and compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e44682-41ec-4eac-a7c8-30c7ba8b6857",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Everyone wants to be a comedian\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "print(joke_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff6eba-d81a-41e4-ab14-f6241f93aecc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the program and run on a familiar GPU\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = joke_prompt | model\n",
    "\n",
    "# Run it => Invoke()\n",
    "topic = \"Software Engineers\"\n",
    "count = 2\n",
    "result = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "\n",
    "# Show output\n",
    "print(result)\n",
    "print_line()\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45f74c-1c0a-47a8-929b-e2e2482349ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compile the program and run on our CPU\n",
    "\n",
    "local_model = GPT4All(model=\"./falcon.bin\")\n",
    "local_chain = joke_prompt | local_model\n",
    "\n",
    "# Run it => Invoke()\n",
    "topic = \"Software Engineers\"\n",
    "count = 2\n",
    "result = local_chain.invoke({\"topic\": topic, \"count\": count})\n",
    "\n",
    "# Show output\n",
    "print_line()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36d355-159f-40f9-8db8-c0827fb77d01",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tangent - why not run on llama2?\n",
    "prompt = joke_prompt.invoke({\"topic\": topic, \"count\": 2})\n",
    "prompt = prompt.messages[0].content\n",
    "\n",
    "# Copy prompt to clipboard and paste into AutoGPT !\n",
    "subprocess.run(f'echo \"{prompt}\" | pbcopy', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e497c0-0bd0-4a0e-a95c-c91463ec1e1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - how did you know how to copy to clipboard?\n",
    "subprocess.run(\n",
    "    [\n",
    "        \"open\",\n",
    "        \"--url\",\n",
    "        \"https://gist.github.com/idvorkin/2ad5517f38c74338d0ab5d33e3ea51b7\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19dcc4-640e-4ffd-8167-263cafd48103",
   "metadata": {},
   "source": [
    "### Functions - The I/O System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c1a76-0863-4aaf-8ca9-f74fa39cd39a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The rain in spain\n",
    "# Tell the model the \"OS\" supports getting the weather\n",
    "\n",
    "\n",
    "# define a callable function\n",
    "class GetWeather(BaseModel):\n",
    "    City: str\n",
    "\n",
    "\n",
    "get_weather = model_to_openai_function(GetWeather)\n",
    "\n",
    "weather_prompt_template = \"What's the weather in {place}\"\n",
    "model = ChatOpenAI()\n",
    "weather_prompt = ChatPromptTemplate.from_template(weather_prompt_template)\n",
    "\n",
    "chain = weather_prompt | model.bind(\n",
    "    functions=[get_weather]  # tell model we can call it.\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Show the full response:\")\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print(response)\n",
    "\n",
    "print(\"Include an output parser in the chain:\")\n",
    "print_function_call(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416bd24-8278-49df-ba06-84b2e592e2fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Woah  - Did you see  the bug?\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Do \"some more programming\"\n",
    "weather_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"When an API takes a city, infer an appropritiate city\"),\n",
    "        HumanMessagePromptTemplate.from_template(weather_prompt_template),\n",
    "    ]\n",
    ")\n",
    "chain = weather_prompt | model.bind(functions=[get_weather])\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print_function_call(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489308bf-a7dc-43fb-adf8-473db4e9adf6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Back to our functions\n",
    "\n",
    "weather_with_data = weather_prompt.copy()\n",
    "\n",
    "# Update prompt with AI's desire to call a function\n",
    "weather_with_data.append(response)\n",
    "\n",
    "# Need to make tomorrow's cut, just stamp this please :)\n",
    "# Will come back and make a dispatcher and call actual functions\n",
    "\n",
    "weather_with_data.append(\n",
    "    FunctionMessage(name=\"GetWeather\", content=\"5 degrees and rainy\")\n",
    ")\n",
    "\n",
    "print(weather_with_data)\n",
    "\n",
    "\n",
    "chain = weather_with_data | model.bind(\n",
    "    functions=[get_weather]\n",
    ")  # tell model we can call it.\n",
    "\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print_line()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430962af-0803-4277-af45-aa1688cfcdfa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - Why do we seperate view from model?\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str\n",
    "    punch_line: str\n",
    "    reason_joke_is_funny: str\n",
    "\n",
    "\n",
    "class GetJokes(BaseModel):\n",
    "    count: int\n",
    "    jokes: List[Joke]\n",
    "\n",
    "\n",
    "get_jokes = model_to_openai_function(GetJokes)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "chain = prompt | model.bind(functions=[get_jokes])\n",
    "\n",
    "print(prompt.messages)\n",
    "\n",
    "response = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "print_line()\n",
    "print_function_call(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a7f7e-eee7-4abe-a325-386fb8d7bcf8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - What's better then doing math with a calculator?\n",
    "\n",
    "\n",
    "solve_math_with_python = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"Write code to solve the users problem. \n",
    "            the last line of the python  program should print the answer. \n",
    "            Only use built in packages\"\"\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"What is the 217th prime\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ExecutePythonCode(BaseModel):\n",
    "    valid_python: str\n",
    "    code_explanation: str\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\").bind(\n",
    "    function_call={\"name\": \"ExecutePythonCode\"},  # tell gpt to use this model\n",
    "    functions=[model_to_openai_function(ExecutePythonCode)],\n",
    ")\n",
    "\n",
    "\n",
    "# JsonOutputFunctionParser2 == PrettyPrintOutput\n",
    "\n",
    "chain = solve_math_with_python | model | JsonOutputFunctionsParser2()\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response[\"code_explanation\"])\n",
    "print_line()\n",
    "\n",
    "valid_python = response[\"valid_python\"]\n",
    "print(valid_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6cb89-edf8-4285-93e1-321a94b7888c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Woah Nelly  ... are you sure you want to do this??\n",
    "print_line()\n",
    "# input(\"Are you sure you want to run this code??\")\n",
    "exec(valid_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873feb1-d320-4e7e-992d-ff32737a33a5",
   "metadata": {},
   "source": [
    "### Memory - Staying in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093eac8-3709-4410-9f24-5a2b54f5a216",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory - # no memory\n",
    "model = ChatOpenAI().bind(temperature=0)\n",
    "user_says = \"Tell me another joke\"\n",
    "last_prompt = \"\"\n",
    "\n",
    "# Pretend this is a conversation, without memory, it's pretty boring\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "    prompt = ChatPromptTemplate.from_messages([user_says])\n",
    "    print(f\"Prompt {i}\\n\", prompt.messages)\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke({})\n",
    "\n",
    "    print(f\"Response {i}\\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa0d47-c408-4514-8463-b9fac7e6bfe4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# The memory module!\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "memory.add_user_message(\"I like software engineering jokes\")\n",
    "memory.add_ai_message(\"OK!\")\n",
    "print(memory.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912f985-8b0c-4bec-b56b-557a672b3b0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory w/Memory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "memory.add_user_message(\"I like software engineering jokes\")\n",
    "memory.add_ai_message(\"OK!\")\n",
    "\n",
    "# human always says the same thing\n",
    "human_says = \"tell me another joke\"\n",
    "prompt = \"\"  # have it outside loop so we can print it\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "\n",
    "    # build prompt, including the memory\n",
    "    # We'll only print the last one ...\n",
    "    prompt = ChatPromptTemplate.from_messages(memory.messages)\n",
    "    prompt.append(human_says)\n",
    "    # run the chain\n",
    "    chain = prompt | model\n",
    "\n",
    "    response = chain.invoke({})\n",
    "    print(f\"Response {i}\\n\", response.content)\n",
    "\n",
    "    # store history\n",
    "    memory.add_user_message(human_says)\n",
    "    memory.add_ai_message(response.content)\n",
    "\n",
    "print_line()\n",
    "print_line()\n",
    "print(f\"Final Prompt\\n\", prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11df5ec-0ebb-47c8-b374-0befb237b82b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - What happens when we run out of space? \n",
    "# A new kind of lossy compression\n",
    "\n",
    "semantic_compression_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"You are a compressing chat model, \n",
    "            summarize the entire conversation into a paragaph\"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for m in memory.messages:\n",
    "    semantic_compression_prompt.append(m)\n",
    "\n",
    "chain = semantic_compression_prompt | model\n",
    "\n",
    "response = chain.invoke({})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca5fe0-7eb8-48b9-8ab9-d9bff6020e7f",
   "metadata": {},
   "source": [
    "### Retrievel and Index - Exceeding RAM and Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc6d0a-c229-45a6-a69e-e41ba9a775ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data -> Index -> Retreive:  A picture is worth a thousand words\n",
    "Image(filename=\"./images/retrieval_pipeline.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53456a5b-a781-4433-a165-57739cbe5bd9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Query some young pets - using the alphabet\n",
    "\n",
    "# Data Set small enough we can use raw data run the query and retrieval in one fell swoop\n",
    "things_we_saw = [\n",
    "    \"dog\",\n",
    "    \"cat\",\n",
    "    \"zebra\",\n",
    "    \"puppy\",\n",
    "    \"calf\",\n",
    "    \"puppies\",\n",
    "    \"kitten\",\n",
    "    \"cow\",\n",
    "    \"desk\",\n",
    "    \"rubber band\",\n",
    "    \"mouse\",\n",
    "    \"airpods\",\n",
    "    \"smelly socks\",\n",
    "    \"sandals\",\n",
    "    \"jacket\",\n",
    "    \"cats\",\n",
    "    \"young dog\",\n",
    "    \"young cat\",\n",
    "    \"young hamster\",\n",
    "]\n",
    "\n",
    "# Find young animals. Pretty hard with just regexp\n",
    "retrieval_query = \"puppy|kitten\"\n",
    "young_pets = [item for item in things_we_saw if re.match(retrieval_query, item)]\n",
    "\n",
    "\n",
    "print(young_pets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e63d61-4706-455c-b28a-8e21d1f55994",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Query some young pets - using english meaning (build cache)\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "\n",
    "# In our brains, we don't think of the word as spelling \n",
    "# It has meaning! How can a computer represent that meaning? \n",
    "# The same way it does everything else, a big vector! \n",
    "\n",
    "# Reperesent words using embedding (if time see pictures)\n",
    "\n",
    "print_line()\n",
    "\n",
    "VectorStore = FAISS\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = VectorStore.from_texts(things_we_saw, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7c490-07eb-45df-a7e9-3780e5b12cfa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Query some young pets - using english meaning (Embeddings)\n",
    "\n",
    "print(db.similarity_search_with_relevance_scores(\"young animals\", k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1866b-9eaf-4ad5-a8ed-030b3fc11e00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate: Did someone have some journal entries?\n",
    "\n",
    "script = \"\"\"\n",
    "mosh lightsail\n",
    "z nlp\n",
    "./igor-journal.py  --help\n",
    "\n",
    "# Look at entries by date, when was covid?\n",
    "\n",
    "./igor_journal.py entries 2020-04-30\n",
    "\n",
    "# Wait when was I most impacted? \n",
    "\n",
    " ./igor_journal.py files_with_word covid\n",
    "\n",
    "# Lets see what was going on that day.\n",
    "\n",
    "./igor_journal.py body 2022-04-10  \n",
    " \n",
    "\n",
    "# Blah, wall of text, lets have Dr. GPT analyze\n",
    "\n",
    "./life.py journal-report 2022-04-10 -u4\n",
    "\n",
    "# Look skim the code\n",
    "vim ~/gits/nlp/life.py \n",
    "\n",
    "# Lets look at that :) - zm to fold \n",
    "vim ~/gits/nlp/assets/report-2022-04-10.md\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a5f20-462b-4c71-90ec-553b536a4b7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# If we have time: Walk through of Word Embeddings\n",
    "# https://investigate.ai/text-analysis/word-embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e39b3c-2971-4083-af42-df5874921a43",
   "metadata": {},
   "source": [
    "### Planning - Lets put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136af2f-a9e3-4b11-8e50-9ccfd768bc1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Help me write a talk\n",
    "notes = \"\"\"\n",
    "- Run auto-gpt\n",
    "- Write a talk on LLMs\n",
    "- Chain of Thought\n",
    "- LLM to run feedback\n",
    "- Agent to Run Commands\n",
    "\"\"\"\n",
    "print(notes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
